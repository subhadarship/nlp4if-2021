03/21/2021 21:40:21 - INFO - __main__ -   

*****************
***RUN STARTED***
*****************

03/21/2021 21:40:21 - INFO - __main__ -   args
-----------------------------------------------------------------------------------------
	srclangs_with_num_samples: ar_all,bg_all
	trglang: bg
	train_data_dir: ../data/prepared
	dev_data_dir: ../data/prepared
	test_data_dir: None
	batch_size: 4096
	max_vocab_size: 16000
	tokenization: tweet
	hid_dim: 128
	num_enc_layers: 3
	num_enc_heads: 8
	enc_pf_dim: 256
	enc_dropout: 0.1
	fc_dim: 64
	bert_fc_dim: 64
	logistic_regression_hid_dim: 128
	logistic_regression_dropout: 0.1
	log_file_path: ../logs/arbg/transformer_encoder/full_hidden128_vocab16000.txt
	random_seed: 123
	lr: 0.0005
	clip: 1.0
	max_epochs: 999
	model_dir: ../models/arbg/transformer_encoder/full_hidden128_vocab16000
	no_xavier_initialization: False
	early_stopping_patience: 10
	model_name: transformer_enc
	freeze_bert: False
-----------------------------------------------------------------------------------------

03/21/2021 21:40:22 - INFO - __main__ -   device: cuda
03/21/2021 21:40:22 - INFO - data_utils.load -   considered 165 (100.00 %) samples out of 165 total samples in ../data/prepared/train.ar.tsv
03/21/2021 21:40:22 - INFO - data_utils.load -   considered 3000 (100.00 %) samples out of 3000 total samples in ../data/prepared/train.bg.tsv
03/21/2021 21:40:22 - INFO - data_utils.load -   considered 350 (100.00 %) samples out of 350 total samples in ../data/prepared/dev.bg.tsv
03/21/2021 21:40:22 - INFO - data_utils.preprocess -   tokenization: tweet
03/21/2021 21:40:22 - INFO - data_utils.field -   16501 (100.00%) tokens out of 16501 tokens are kept in vocabulary
03/21/2021 21:40:23 - WARNING - data_utils.data -   trimming sentence 1933 of length 1659 to 1000 tokens (trimmed tokens include <sos> and None tokens)
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   num train samples: 3165
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   num val samples: 350
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   num test samples: None
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   train sentence max len: 1000
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   val sentence max len: 62
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   train OOV: 0 (0.00%) out of 80390 running tokens are OOV
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   val OOV: 797 (10.85%) out of 7349 running tokens are OOV
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   look at some train samples 👀
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   sample idx: 0, original text: وزائرتي كأن بها حياء فليس تزور إلا في الظلام فرشت لها المطارف والحشايا فعافتها وباتت في عظامي يضيق الجلد عن نفسي وعنها فتوسعه بأنواع السقام اذا ما فارقتني غسلتني كأنا عاكفان على حرام #المتنبي #الحمى #وباء #كورونا #الكويت #السعودية #قطر #الامارات #البحرين #عمان URL, preprocessed text: ['<sos>', 'وزائرتي', 'كأن', 'بها', 'حياء', 'فليس', 'تزور', 'إلا', 'في', 'الظلام', 'فرشت', 'لها', 'المطارف', 'والحشايا', 'فعافتها', 'وباتت', 'في', 'عظامي', 'يضيق', 'الجلد', 'عن', 'نفسي', 'وعنها', 'فتوسعه', 'بأنواع', 'السقام', 'اذا', 'ما', 'فارقتني', 'غسلتني', 'كأنا', 'عاكفان', 'على', 'حرام', '#المتنبي', '#الحمى', '#وباء', '#كورونا', '#الكويت', '#السعودية', '#قطر', '#الامارات', '#البحرين', '#عمان', 'URL'], text ids: [2, 5483, 3122, 952, 5484, 5485, 5486, 852, 47, 5487, 5488, 2167, 5489, 5490, 5491, 5492, 47, 5493, 5494, 5495, 174, 5496, 5497, 5498, 5499, 5500, 3123, 515, 5501, 5502, 5503, 5504, 133, 5505, 5506, 5507, 5508, 75, 5509, 347, 374, 5510, 5511, 5512, 63], original labels: no nan nan nan nan no no, label ids: [[1], [2], [2], [2], [2], [1], [1]]
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   sample idx: 1, original text: بأمر خادم الحرمين الشريفين  منع التجول للحد  من انتشار #فيروس_كورونا  من الساعة 7م حتى الساعة 6ص  لمدة 21 يوم URL, preprocessed text: ['<sos>', 'بأمر', 'خادم', 'الحرمين', 'الشريفين', 'منع', 'التجول', 'للحد', 'من', 'انتشار', '#فيروس_كورونا', 'من', 'الساعة', '7م', 'حتى', 'الساعة', '6ص', 'لمدة', '21', 'يوم', 'URL'], text ids: [2, 5513, 1623, 1624, 2168, 2169, 445, 446, 35, 291, 423, 35, 311, 5514, 759, 311, 5515, 679, 245, 560, 63], original labels: yes no yes no no no no, label ids: [[0], [1], [0], [1], [1], [1], [1]]
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   sample idx: 2, original text: الحيوانات تغزو المدن بعد تطبيق إجراءات حظر التجوال للوقاية من فيروس كورونا حول العالم، صور من ايطاليا و اليابان و جزيرة سردينيا .. URL, preprocessed text: ['<sos>', 'الحيوانات', 'تغزو', 'المدن', 'بعد', 'تطبيق', 'إجراءات', 'حظر', 'التجوال', 'للوقاية', 'من', 'فيروس', 'كورونا', 'حول', 'العالم', '،', 'صور', 'من', 'ايطاليا', 'و', 'اليابان', 'و', 'جزيرة', 'سردينيا', '..', 'URL'], text ids: [2, 3124, 5516, 5517, 618, 3125, 2170, 953, 5518, 2171, 35, 246, 104, 1625, 398, 67, 5519, 35, 3126, 399, 5520, 399, 5521, 5522, 58, 63], original labels: yes yes nan yes yes yes yes, label ids: [[0], [0], [2], [0], [0], [0], [0]]
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   sample idx: 3, original text: تواجه قناتي: (يوسف علاونة مباشر) حملة شعواء من بهايم حلف اللطم وتضمن هذا دفع رشاوى لتعطيل الاشتراكات القديمة  برجاء تجديد الاشتراك وتفعيل الجرس  ونشر هذه التغريدة #كورونا #اوامر_ملكية #Covid_19 #قطر #ترك #مجوس #اخوان #احذية #روافض #خوارج #يوسف_علاونة  URL, preprocessed text: ['<sos>', 'تواجه', 'قناتي', ':', '(', 'يوسف', 'علاونة', 'مباشر', ')', 'حملة', 'شعواء', 'من', 'بهايم', 'حلف', 'اللطم', 'وتضمن', 'هذا', 'دفع', 'رشاوى', 'لتعطيل', 'الاشتراكات', 'القديمة', 'برجاء', 'تجديد', 'الاشتراك', 'وتفعيل', 'الجرس', 'ونشر', 'هذه', 'التغريدة', '#كورونا', '#اوامر_ملكية', '#Covid_19', '#قطر', '#ترك', '#مجوس', '#اخوان', '#احذية', '#روافض', '#خوارج', '#يوسف_علاونة', 'URL'], text ids: [2, 3127, 5523, 16, 30, 5524, 5525, 5526, 31, 5527, 5528, 35, 5529, 5530, 5531, 5532, 561, 5533, 5534, 5535, 5536, 3128, 5537, 5538, 5539, 5540, 5541, 3129, 516, 2172, 75, 5542, 375, 374, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 63], original labels: no nan nan nan nan no no, label ids: [[1], [2], [2], [2], [2], [1], [1]]
03/21/2021 21:40:23 - INFO - data_utils.preprocess -   sample idx: 4, original text: إصابة عاملين بمشروعات كأس العالم في قطر بـ #كورونا #العربية_عاجل URL, preprocessed text: ['<sos>', 'إصابة', 'عاملين', 'بمشروعات', 'كأس', 'العالم', 'في', 'قطر', 'بـ', '#كورونا', '#العربية_عاجل', 'URL'], text ids: [2, 326, 5550, 5551, 5552, 398, 47, 327, 1626, 75, 3130, 63], original labels: yes yes yes yes yes yes yes, label ids: [[0], [0], [0], [0], [0], [0], [0]]
03/21/2021 21:40:25 - INFO - data_utils.preprocess -   there are nearly 26 batches in an epoch
03/21/2021 21:40:25 - INFO - __main__ -   model
-----------------------------------------------------------------------------------------
MultitaskTransformerEncoderClassificationModel(
  (encoder): Encoder(
    (tok_embedding): Embedding(16504, 128)
    (pos_embedding): Embedding(1000, 128)
    (layers): ModuleList(
      (0): EncoderLayer(
        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttentionLayer(
          (fc_q): Linear(in_features=128, out_features=128, bias=True)
          (fc_k): Linear(in_features=128, out_features=128, bias=True)
          (fc_v): Linear(in_features=128, out_features=128, bias=True)
          (fc): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (positionwise_feedforward): PositionwiseFeedforwardLayer(
          (fc_1): Linear(in_features=128, out_features=256, bias=True)
          (fc_2): Linear(in_features=256, out_features=128, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (dropout): Dropout(p=0.1)
      )
      (1): EncoderLayer(
        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttentionLayer(
          (fc_q): Linear(in_features=128, out_features=128, bias=True)
          (fc_k): Linear(in_features=128, out_features=128, bias=True)
          (fc_v): Linear(in_features=128, out_features=128, bias=True)
          (fc): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (positionwise_feedforward): PositionwiseFeedforwardLayer(
          (fc_1): Linear(in_features=128, out_features=256, bias=True)
          (fc_2): Linear(in_features=256, out_features=128, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (dropout): Dropout(p=0.1)
      )
      (2): EncoderLayer(
        (layer_norm): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)
        (self_attention): SelfAttentionLayer(
          (fc_q): Linear(in_features=128, out_features=128, bias=True)
          (fc_k): Linear(in_features=128, out_features=128, bias=True)
          (fc_v): Linear(in_features=128, out_features=128, bias=True)
          (fc): Linear(in_features=128, out_features=128, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (positionwise_feedforward): PositionwiseFeedforwardLayer(
          (fc_1): Linear(in_features=128, out_features=256, bias=True)
          (fc_2): Linear(in_features=256, out_features=128, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (dropout): Dropout(p=0.1)
      )
    )
    (dropout): Dropout(p=0.1)
  )
  (fc): Linear(in_features=128, out_features=64, bias=True)
  (clfs): ModuleList(
    (0): Linear(in_features=64, out_features=3, bias=True)
    (1): Linear(in_features=64, out_features=3, bias=True)
    (2): Linear(in_features=64, out_features=3, bias=True)
    (3): Linear(in_features=64, out_features=3, bias=True)
    (4): Linear(in_features=64, out_features=3, bias=True)
    (5): Linear(in_features=64, out_features=3, bias=True)
    (6): Linear(in_features=64, out_features=3, bias=True)
  )
)
-----------------------------------------------------------------------------------------

03/21/2021 21:40:25 - INFO - __main__ -   the model has 2,646,805 trainable parameters
03/21/2021 21:40:25 - INFO - __main__ -   applying xavier initialization of model parameters
03/21/2021 21:40:25 - INFO - __main__ -   🌋  starting training..
03/21/2021 21:40:26 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:26 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:26 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:26 - INFO - training_utils.postprocess -   1 (0.07 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:26 - INFO - __main__ -   Epoch: 0001 | Time: 0m_1s | train_loss: 5.350 | val_loss: 3.250
03/21/2021 21:40:26 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.8444897959183673, 'f1': 0.81527869182032, 'precision': 0.81527869182032, 'recall': 0.81527869182032}
03/21/2021 21:40:26 - INFO - __main__ -   	--Found new best val f1
03/21/2021 21:40:27 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:27 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:27 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:27 - INFO - training_utils.postprocess -   2 (0.14 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:27 - INFO - __main__ -   Epoch: 0002 | Time: 0m_1s | train_loss: 2.985 | val_loss: 4.092
03/21/2021 21:40:27 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7975510204081633, 'f1': 0.7889195836555167, 'precision': 0.7889195836555167, 'recall': 0.7889195836555167}
03/21/2021 21:40:28 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:28 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:28 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:28 - INFO - training_utils.postprocess -   58 (4.14 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:28 - INFO - __main__ -   Epoch: 0003 | Time: 0m_1s | train_loss: 1.859 | val_loss: 5.280
03/21/2021 21:40:28 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7587755102040816, 'f1': 0.7666237217368674, 'precision': 0.7666237217368674, 'recall': 0.7666237217368674}
03/21/2021 21:40:29 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:29 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:29 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:29 - INFO - training_utils.postprocess -   122 (8.71 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:29 - INFO - __main__ -   Epoch: 0004 | Time: 0m_0s | train_loss: 1.471 | val_loss: 5.530
03/21/2021 21:40:29 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7583673469387755, 'f1': 0.7648188707128141, 'precision': 0.7648188707128141, 'recall': 0.7648188707128141}
03/21/2021 21:40:30 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:30 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:30 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:30 - INFO - training_utils.postprocess -   14 (1.00 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:30 - INFO - __main__ -   Epoch: 0005 | Time: 0m_0s | train_loss: 1.100 | val_loss: 4.591
03/21/2021 21:40:30 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.8081632653061224, 'f1': 0.8140553560841359, 'precision': 0.8140553560841359, 'recall': 0.8140553560841359}
03/21/2021 21:40:31 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:31 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:31 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:31 - INFO - training_utils.postprocess -   29 (2.07 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:31 - INFO - __main__ -   Epoch: 0006 | Time: 0m_0s | train_loss: 0.674 | val_loss: 5.569
03/21/2021 21:40:31 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7791836734693878, 'f1': 0.8023514717660066, 'precision': 0.8023514717660066, 'recall': 0.8023514717660066}
03/21/2021 21:40:32 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:32 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:32 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:32 - INFO - training_utils.postprocess -   20 (1.43 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:32 - INFO - __main__ -   Epoch: 0007 | Time: 0m_1s | train_loss: 0.509 | val_loss: 5.690
03/21/2021 21:40:32 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7620408163265306, 'f1': 0.790310985039789, 'precision': 0.790310985039789, 'recall': 0.790310985039789}
03/21/2021 21:40:33 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:33 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:33 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:33 - INFO - training_utils.postprocess -   43 (3.07 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:33 - INFO - __main__ -   Epoch: 0008 | Time: 0m_0s | train_loss: 0.440 | val_loss: 6.853
03/21/2021 21:40:33 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7873469387755103, 'f1': 0.7968035247003881, 'precision': 0.7968035247003881, 'recall': 0.7968035247003881}
03/21/2021 21:40:34 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:34 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:34 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:34 - INFO - training_utils.postprocess -   35 (2.50 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:34 - INFO - __main__ -   Epoch: 0009 | Time: 0m_1s | train_loss: 0.232 | val_loss: 7.281
03/21/2021 21:40:34 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7840816326530612, 'f1': 0.7999401896413286, 'precision': 0.7999401896413286, 'recall': 0.7999401896413286}
03/21/2021 21:40:35 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:35 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:35 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:35 - INFO - training_utils.postprocess -   73 (5.21 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:35 - INFO - __main__ -   Epoch: 0010 | Time: 0m_1s | train_loss: 0.232 | val_loss: 7.747
03/21/2021 21:40:35 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7726530612244897, 'f1': 0.7889709915430981, 'precision': 0.7889709915430981, 'recall': 0.7889709915430981}
03/21/2021 21:40:36 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:36 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:36 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:36 - INFO - training_utils.postprocess -   94 (6.71 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:36 - INFO - __main__ -   Epoch: 0011 | Time: 0m_0s | train_loss: 0.221 | val_loss: 9.077
03/21/2021 21:40:36 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.7114285714285715, 'f1': 0.7513194610971258, 'precision': 0.7513194610971258, 'recall': 0.7513194610971258}
03/21/2021 21:40:36 - INFO - __main__ -   	--STOPPING EARLY
03/21/2021 21:40:36 - INFO - __main__ -   load checkpoint from ../models/arbg/transformer_encoder/full_hidden128_vocab16000
03/21/2021 21:40:36 - INFO - __main__ -   load model weights from checkpoint in ../models/arbg/transformer_encoder/full_hidden128_vocab16000
03/21/2021 21:40:36 - INFO - training_utils.train_loop -   postprocessing targets..
03/21/2021 21:40:36 - INFO - training_utils.postprocess -   13 (0.93 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:36 - INFO - training_utils.train_loop -   postprocessing predictions..
03/21/2021 21:40:36 - INFO - training_utils.postprocess -   1 (0.07 %) out of 1400 q2, q3, q4, q5 predictions are changed during postprocessing
03/21/2021 21:40:36 - INFO - __main__ -   best_val_loss: 3.250
03/21/2021 21:40:36 - INFO - __main__ -   📣 best validation metrics 📣 {'acc': 0.8444897959183673, 'f1': 0.81527869182032, 'precision': 0.81527869182032, 'recall': 0.81527869182032}
