03/18/2021 18:59:30 - INFO - __main__ -   

*****************
***RUN STARTED***
*****************

03/18/2021 18:59:30 - INFO - __main__ -   args
-----------------------------------------------------------------------------------------
	srclangs_with_num_samples: ar_all
	trglang: ar
	train_data_dir: ../data/prepared
	dev_data_dir: ../data/prepared
	test_data_dir: None
	batch_size: 1024
	max_vocab_size: None
	tokenization: bert-base-multilingual-cased
	hid_dim: 128
	num_enc_layers: 3
	num_enc_heads: 8
	enc_pf_dim: 256
	enc_dropout: 0.1
	fc_dim: 64
	bert_fc_dim: 256
	logistic_regression_hid_dim: 128
	logistic_regression_dropout: 0.1
	log_file_path: ../logs/ar/bert-base-multilingual-cased/trg_fc256_lr0.0005_frozen.txt
	random_seed: 123
	lr: 0.0005
	clip: 1.0
	max_epochs: 999
	model_dir: /mnt/backup/panda/nlp4if-2021/models/ar/bert-base-multilingual-cased/trg_fc256_lr0.0005_frozen
	no_xavier_initialization: False
	early_stopping_patience: 10
	model_name: bert-base-multilingual-cased
	freeze_bert: True
-----------------------------------------------------------------------------------------

03/18/2021 18:59:31 - INFO - __main__ -   device: cuda
03/18/2021 18:59:31 - INFO - data_utils.load -   considered 165 (100.00 %) samples out of 165 total samples in ../data/prepared/train.ar.tsv
03/18/2021 18:59:31 - INFO - data_utils.load -   considered 33 (100.00 %) samples out of 33 total samples in ../data/prepared/dev.ar.tsv
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   tokenization: bert-base-multilingual-cased
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   num train samples: 165
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   num val samples: 33
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   num test samples: None
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   train sentence max len: 143
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   val sentence max len: 122
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   train OOV: 117 (0.89%) out of 13175 running tokens are OOV
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   val OOV: 16 (0.59%) out of 2704 running tokens are OOV
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   look at some train samples 👀
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   sample idx: 0, original text: وزائرتي كأن بها حياء فليس تزور إلا في الظلام فرشت لها المطارف والحشايا فعافتها وباتت في عظامي يضيق الجلد عن نفسي وعنها فتوسعه بأنواع السقام اذا ما فارقتني غسلتني كأنا عاكفان على حرام #المتنبي #الحمى #وباء #كورونا #الكويت #السعودية #قطر #الامارات #البحرين #عمان URL, text ids: [101, 791, 54252, 31944, 26614, 786, 35849, 10582, 21016, 77887, 12700, 784, 20109, 11091, 766, 43884, 10673, 22918, 10210, 59901, 36793, 36334, 784, 67151, 10502, 22543, 59901, 10700, 37420, 11687, 791, 13154, 12616, 70935, 14431, 784, 54422, 64579, 10429, 88868, 10564, 10502, 10210, 781, 43931, 10461, 793, 48711, 11852, 59901, 77191, 11749, 32254, 10461, 791, 11693, 79702, 784, 40041, 82397, 10388, 33844, 14556, 11693, 59901, 11091, 36443, 763, 38901, 12441, 784, 11884, 39053, 24148, 782, 32219, 28089, 10461, 786, 35849, 14695, 781, 40446, 94523, 10560, 769, 42519, 108, 59901, 34783, 10582, 22908, 108, 59901, 86131, 11832, 108, 88868, 12700, 108, 786, 52274, 14695, 108, 62329, 108, 38658, 108, 58705, 108, 59901, 70701, 23523, 108, 95721, 108, 54299, 31191, 102], original labels: no nan nan nan nan no no, label ids: [[1], [2], [2], [2], [2], [1], [1]]
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   sample idx: 1, original text: بأمر خادم الحرمين الشريفين  منع التجول للحد  من انتشار #فيروس_كورونا  من الساعة 7م حتى الساعة 6ص  لمدة 21 يوم URL, text ids: [101, 764, 80358, 10673, 770, 13761, 10700, 59901, 102562, 11294, 59901, 31330, 20884, 11294, 10289, 11693, 59901, 51731, 14358, 787, 87536, 10658, 10289, 49520, 108, 10210, 53797, 168, 786, 52274, 14695, 10289, 100736, 128, 10700, 15164, 100736, 127, 15470, 52237, 10296, 26566, 31191, 102], original labels: yes no yes no no no no, label ids: [[0], [1], [0], [1], [1], [1], [1]]
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   sample idx: 2, original text: الحيوانات تغزو المدن بعد تطبيق إجراءات حظر التجوال للوقاية من فيروس كورونا حول العالم، صور من ايطاليا و اليابان و جزيرة سردينيا .. URL, text ids: [101, 81903, 766, 17329, 43884, 74140, 11866, 766, 86765, 761, 70520, 10564, 769, 34353, 59901, 51731, 37172, 787, 29426, 41003, 10535, 10289, 10210, 53797, 786, 52274, 14695, 25705, 19300, 752, 777, 12379, 10289, 763, 81177, 10429, 791, 83209, 791, 58512, 28340, 44316, 14431, 119, 119, 31191, 102], original labels: yes yes nan yes yes yes yes, label ids: [[0], [0], [2], [0], [0], [0], [0]]
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   sample idx: 3, original text: تواجه قناتي: (يوسف علاونة مباشر) حملة شعواء من بهايم حلف اللطم وتضمن هذا دفع رشاوى لتعطيل الاشتراكات القديمة  برجاء تجديد الاشتراك وتفعيل الجرس  ونشر هذه التغريدة #كورونا #اوامر_ملكية #Covid_19 #قطر #ترك #مجوس #اخوان #احذية #روافض #خوارج #يوسف_علاونة  URL, text ids: [101, 23458, 24728, 10388, 785, 68329, 10461, 131, 113, 57524, 781, 20451, 68978, 788, 105153, 114, 55620, 10382, 776, 98273, 12700, 10289, 21016, 20556, 51706, 11687, 59901, 10961, 14286, 10700, 791, 71415, 13498, 101035, 773, 70935, 38776, 787, 10502, 11693, 54210, 10961, 59901, 35155, 16506, 40446, 10564, 46416, 72798, 12700, 766, 26897, 14472, 59901, 35155, 16506, 40446, 791, 10502, 46193, 15951, 59901, 24618, 11091, 791, 66205, 13159, 59901, 10502, 17329, 67075, 10382, 108, 786, 52274, 14695, 108, 12084, 13367, 10673, 168, 61492, 10535, 108, 13098, 32194, 168, 10270, 108, 58705, 108, 54237, 12497, 108, 788, 54731, 11091, 108, 763, 79963, 108, 763, 12616, 22973, 10535, 108, 55532, 18562, 15386, 108, 770, 26725, 13027, 108, 57524, 168, 781, 20451, 68978, 31191, 102], original labels: no nan nan nan nan no no, label ids: [[1], [2], [2], [2], [2], [1], [1]]
03/18/2021 18:59:31 - INFO - data_utils.preprocess_bert -   sample idx: 4, original text: إصابة عاملين بمشروعات كأس العالم في قطر بـ #كورونا #العربية_عاجل URL, text ids: [101, 761, 58863, 10382, 61927, 11294, 764, 10700, 31330, 96262, 36924, 19300, 10210, 58705, 20496, 108, 786, 52274, 14695, 108, 19179, 168, 781, 24728, 10961, 31191, 102], original labels: yes yes yes yes yes yes yes, label ids: [[0], [0], [0], [0], [0], [0], [0]]
03/18/2021 18:59:33 - INFO - data_utils.preprocess_bert -   there are nearly 15 batches in an epoch
03/18/2021 18:59:36 - INFO - __main__ -   model
-----------------------------------------------------------------------------------------
MultitaskBertClassificationModel(
  (encoder): BERT(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(119547, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (fc): Linear(in_features=768, out_features=256, bias=True)
  (clfs): ModuleList(
    (0): Linear(in_features=256, out_features=3, bias=True)
    (1): Linear(in_features=256, out_features=3, bias=True)
    (2): Linear(in_features=256, out_features=3, bias=True)
    (3): Linear(in_features=256, out_features=3, bias=True)
    (4): Linear(in_features=256, out_features=3, bias=True)
    (5): Linear(in_features=256, out_features=3, bias=True)
    (6): Linear(in_features=256, out_features=3, bias=True)
  )
)
-----------------------------------------------------------------------------------------

03/18/2021 18:59:36 - INFO - __main__ -   the model has 202,261 trainable parameters
03/18/2021 18:59:36 - INFO - __main__ -   🌋  starting training..
03/18/2021 18:59:37 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 18:59:37 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:37 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 18:59:37 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:37 - INFO - __main__ -   Epoch: 0001 | Time: 0m_1s | train_loss: 6.902 | val_loss: 6.372
03/18/2021 18:59:37 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5541125541125541, 'f1': 0.46347456265424275, 'precision': 0.46347456265424275, 'recall': 0.46347456265424275}
03/18/2021 18:59:37 - INFO - __main__ -   	--Found new best val f1
03/18/2021 18:59:39 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 18:59:39 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:39 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 18:59:39 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:39 - INFO - __main__ -   Epoch: 0002 | Time: 0m_0s | train_loss: 6.202 | val_loss: 6.284
03/18/2021 18:59:39 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5454545454545455, 'f1': 0.44865454273759536, 'precision': 0.44865454273759536, 'recall': 0.44865454273759536}
03/18/2021 18:59:40 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 18:59:40 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:40 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 18:59:40 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:40 - INFO - __main__ -   Epoch: 0003 | Time: 0m_0s | train_loss: 5.782 | val_loss: 5.788
03/18/2021 18:59:40 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5670995670995671, 'f1': 0.5010767122196559, 'precision': 0.5010767122196559, 'recall': 0.5010767122196559}
03/18/2021 18:59:40 - INFO - __main__ -   	--Found new best val f1
03/18/2021 18:59:51 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 18:59:51 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:51 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 18:59:51 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 18:59:51 - INFO - __main__ -   Epoch: 0004 | Time: 0m_1s | train_loss: 5.529 | val_loss: 5.912
03/18/2021 18:59:51 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5844155844155844, 'f1': 0.5189333371406153, 'precision': 0.5189333371406153, 'recall': 0.5189333371406153}
03/18/2021 18:59:51 - INFO - __main__ -   	--Found new best val f1
03/18/2021 19:00:04 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:04 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:04 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:04 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:04 - INFO - __main__ -   Epoch: 0005 | Time: 0m_0s | train_loss: 5.510 | val_loss: 5.876
03/18/2021 19:00:04 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5627705627705628, 'f1': 0.4899553697143993, 'precision': 0.4899553697143993, 'recall': 0.4899553697143993}
03/18/2021 19:00:05 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:05 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:05 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:05 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:05 - INFO - __main__ -   Epoch: 0006 | Time: 0m_0s | train_loss: 5.415 | val_loss: 5.786
03/18/2021 19:00:05 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5324675324675324, 'f1': 0.4338475377970651, 'precision': 0.4338475377970651, 'recall': 0.4338475377970651}
03/18/2021 19:00:06 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:06 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:06 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:06 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:06 - INFO - __main__ -   Epoch: 0007 | Time: 0m_0s | train_loss: 5.227 | val_loss: 5.691
03/18/2021 19:00:06 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5627705627705628, 'f1': 0.48461856255129426, 'precision': 0.48461856255129426, 'recall': 0.48461856255129426}
03/18/2021 19:00:07 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:07 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:07 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:07 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:07 - INFO - __main__ -   Epoch: 0008 | Time: 0m_0s | train_loss: 5.305 | val_loss: 5.643
03/18/2021 19:00:07 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5714285714285714, 'f1': 0.5041279146740668, 'precision': 0.5041279146740668, 'recall': 0.5041279146740668}
03/18/2021 19:00:08 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:08 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:08 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:08 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:08 - INFO - __main__ -   Epoch: 0009 | Time: 0m_0s | train_loss: 5.195 | val_loss: 5.276
03/18/2021 19:00:08 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.6450216450216449, 'f1': 0.5648708089076251, 'precision': 0.5648708089076251, 'recall': 0.5648708089076251}
03/18/2021 19:00:08 - INFO - __main__ -   	--Found new best val f1
03/18/2021 19:00:18 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:18 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:18 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:18 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:18 - INFO - __main__ -   Epoch: 0010 | Time: 0m_0s | train_loss: 4.913 | val_loss: 5.740
03/18/2021 19:00:18 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.5714285714285714, 'f1': 0.5011960801292622, 'precision': 0.5011960801292622, 'recall': 0.5011960801292622}
03/18/2021 19:00:19 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:19 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:19 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:19 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:19 - INFO - __main__ -   Epoch: 0011 | Time: 0m_0s | train_loss: 4.987 | val_loss: 5.120
03/18/2021 19:00:19 - INFO - __main__ -   📣 val metrics 📣 {'acc': 0.658008658008658, 'f1': 0.6230783571822769, 'precision': 0.6230783571822769, 'recall': 0.6230783571822769}
03/18/2021 19:00:19 - INFO - __main__ -   	--Found new best val f1
03/18/2021 19:00:30 - INFO - __main__ -   	--STOPPING EARLY
03/18/2021 19:00:30 - INFO - __main__ -   load checkpoint from /mnt/backup/panda/nlp4if-2021/models/ar/bert-base-multilingual-cased/trg_fc256_lr0.0005_frozen
03/18/2021 19:00:30 - INFO - __main__ -   load model weights from checkpoint in /mnt/backup/panda/nlp4if-2021/models/ar/bert-base-multilingual-cased/trg_fc256_lr0.0005_frozen
03/18/2021 19:00:31 - INFO - training_utils.train_loop -   postprocessing targets..
03/18/2021 19:00:31 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:31 - INFO - training_utils.train_loop -   postprocessing predictions..
03/18/2021 19:00:31 - INFO - training_utils.postprocess -   0 (0.00 %) out of 132 q2, q3, q4, q5 predictions are changed during postprocessing
03/18/2021 19:00:31 - INFO - __main__ -   best_val_loss: 5.120
03/18/2021 19:00:31 - INFO - __main__ -   📣 best validation metrics 📣 {'acc': 0.658008658008658, 'f1': 0.6230783571822769, 'precision': 0.6230783571822769, 'recall': 0.6230783571822769}
